{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '../'\n",
    "EMBEDDING_DIR = BASE_DIR + 'embeddings/' # http://nlp.stanford.edu/projects/glove/ pretrained vectors\n",
    "TEXT_DATA_DIR = BASE_DIR + '../data/'\n",
    "TEXT_DATA_FILE = \"reviews_rt_all.csv\"\n",
    "HEADER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\") as f:\n",
    "        if HEADER:\n",
    "            header = next(f)\n",
    "        for line in f:\n",
    "            temp_y, temp_x = line.rstrip(\"\\n\").split(\"|\")\n",
    "            data.append(temp_x)\n",
    "            labels.append(temp_y)\n",
    "            \n",
    "    return data, labels\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, np.asarray(labels, dtype='int8'), test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(tokenizer, train, test):\n",
    "    sequences_train = tokenizer.texts_to_sequences(train) # transform words to its indexes\n",
    "    sequences_test = tokenizer.texts_to_sequences(test)\n",
    "    \n",
    "    word_index = tokenizer.word_index # dictionary of word:index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    data_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH) # transform a list to numpy array with shape (nb_samples, MAX_SEQUENCE_LENGTH)\n",
    "    data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)   # be careful because it takes only last MAX_SEQUENCE_LENGTH words\n",
    "    \n",
    "    return data_train, data_test, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56422 unique tokens.\n",
      "Shape of data train tensor: (82088, 50)\n",
      "Shape of data test tensor: (20522, 50)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS) # create dictionary of MAX_NB_WORDS, other words will not be used\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "X_train, X_test, word_index = transform(tokenizer, data_train, data_test)\n",
    "\n",
    "y_train, y_test = to_categorical(np.asarray(labels_train)), to_categorical(np.asarray(labels_test))\n",
    "print('Shape of data train tensor:', X_train.shape)\n",
    "print('Shape of data test tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_w2v():\n",
    "    _fname = \"../embeddings/GoogleNews-vectors-negative300.bin\"\n",
    "    w2vModel = models.KeyedVectors.load_word2vec_format(_fname, binary=True)\n",
    "    return w2vModel \n",
    "embeddings = load_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = embeddings.word_vec(word)\n",
    "    except: \n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_1 = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=False, write_images=True)\n",
    "callback_2 = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "callback_3 = ModelCheckpoint(\"model/lstm_model.hdf5\", monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82088 samples, validate on 20522 samples\n",
      "INFO:tensorflow:Summary name embedding_1_W:0 is illegal; using embedding_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name embedding_1_W:0 is illegal; using embedding_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_i:0 is illegal; using lstm_1_W_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_i:0 is illegal; using lstm_1_W_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_i:0 is illegal; using lstm_1_U_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_i:0 is illegal; using lstm_1_U_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_i:0 is illegal; using lstm_1_b_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_i:0 is illegal; using lstm_1_b_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_c:0 is illegal; using lstm_1_W_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_c:0 is illegal; using lstm_1_W_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_c:0 is illegal; using lstm_1_U_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_c:0 is illegal; using lstm_1_U_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_c:0 is illegal; using lstm_1_b_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_c:0 is illegal; using lstm_1_b_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_f:0 is illegal; using lstm_1_W_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_f:0 is illegal; using lstm_1_W_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_f:0 is illegal; using lstm_1_U_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_f:0 is illegal; using lstm_1_U_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_f:0 is illegal; using lstm_1_b_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_f:0 is illegal; using lstm_1_b_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_o:0 is illegal; using lstm_1_W_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_o:0 is illegal; using lstm_1_W_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_o:0 is illegal; using lstm_1_U_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_o:0 is illegal; using lstm_1_U_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_o:0 is illegal; using lstm_1_b_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_o:0 is illegal; using lstm_1_b_o_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n",
      "Epoch 1/100\n",
      "26240/82088 [========>.....................] - ETA: 128s - loss: 0.5619 - acc: 0.7050 - fmeasure: 0.7045"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', \"fmeasure\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data = [X_test, y_test], batch_size=128, nb_epoch=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
